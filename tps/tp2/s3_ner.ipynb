{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance d'entit√©s nomm√©es avec SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La documentation est accessible ici: https://spacy.io/api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 14:39:06.307063: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-04 14:39:07.936275: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "DEPRECATION: https://github.com/explosion/spacy-models/releases/download/-fr_core_news_md/-fr_core_news_md.tar.gz#egg===fr_core_news_md contains an egg fragment with a non-PEP 508 name. pip 25.3 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/13157\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '==fr_core_news_md': Expected package name at the start of dependency specifier\n",
      "    ==fr_core_news_md\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "!python -m spacy download fr_core_news_md\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconnaissance des entit√©s dans le corpus de 1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Bloc 1 trait√© (200,000 caract√®res, 3899 entit√©s)\n",
      "üíæ Bloc 2 trait√© (200,000 caract√®res, 2806 entit√©s)\n",
      "üíæ Bloc 3 trait√© (200,000 caract√®res, 3654 entit√©s)\n",
      "üíæ Bloc 4 trait√© (200,000 caract√®res, 4579 entit√©s)\n",
      "üíæ Bloc 5 trait√© (200,000 caract√®res, 5243 entit√©s)\n",
      "üíæ Bloc 6 trait√© (200,000 caract√®res, 5879 entit√©s)\n",
      "üíæ Bloc 7 trait√© (200,000 caract√®res, 4056 entit√©s)\n",
      "üíæ Bloc 8 trait√© (200,000 caract√®res, 4952 entit√©s)\n",
      "üíæ Bloc 9 trait√© (200,000 caract√®res, 5248 entit√©s)\n",
      "üíæ Bloc 10 trait√© (200,000 caract√®res, 5564 entit√©s)\n",
      "üíæ Bloc 11 trait√© (200,000 caract√®res, 3547 entit√©s)\n",
      "üíæ Bloc 12 trait√© (200,000 caract√®res, 3691 entit√©s)\n",
      "üíæ Bloc 13 trait√© (200,000 caract√®res, 4940 entit√©s)\n",
      "üíæ Bloc 14 trait√© (200,000 caract√®res, 4666 entit√©s)\n",
      "üíæ Bloc 15 trait√© (109,828 caract√®res, 1876 entit√©s)\n",
      "\n",
      "‚úÖ Analyse termin√©e : 64,600 entit√©s reconnues.\n",
      "üíæ R√©sultat enregistr√© dans : C:\\Users\\tommy\\TAC2\\TAC\\data\\txt\\entites_1965.csv\n"
     ]
    }
   ],
   "source": [
    "annee_choisie = \"1965\"\n",
    "folder_path = r\"C:\\Users\\tommy\\TAC2\\TAC\\data\\txt\"\n",
    "corpus_file = os.path.join(folder_path, f\"corpus_{annee_choisie}.txt\")\n",
    "output_file = os.path.join(folder_path, f\"entites_{annee_choisie}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Charger le texte complet de 1965 --> version trop longue !!! et sans sauvegarde !!!\n",
    "# corpus_file = os.path.join(folder_path, f\"corpus_{annee_choisie}.txt\")\n",
    "# with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "  #  texte = f.read()\n",
    "\n",
    "# Analyse linguistique compl√®te du corpus\n",
    "# nlp.max_length = 3_000_000  # ‚Üê augmente la limite √† 3 millions de caract√®res\n",
    "# doc = nlp(texte)\n",
    "\n",
    "# D√©couper le texte en phrases\n",
    "# sentences = list(doc.sents)\n",
    "\n",
    "# Parcourir chaque phrase et extraire les entit√©s\n",
    "# for sent in sentences:\n",
    "    # entities = [f\"{ent.text} ({ent.label_})\" for ent in sent.ents]\n",
    "    # if entities:\n",
    "        # print(f\"‚Üí '{sent.text.strip()}' contient les entit√©s : {', '.join(entities)}\")\n",
    "\n",
    "# refaire en d√©coupant en blocs pour gagner du temps\n",
    "# === Param√®tres ===\n",
    "# === Charger le mod√®le spaCy (NER uniquement) ===\n",
    "nlp = spacy.load(\"fr_core_news_md\", disable=[\"tagger\", \"parser\", \"lemmatizer\", \"attribute_ruler\"])\n",
    "nlp.max_length = 3_000_000\n",
    "\n",
    "# === Lire le texte complet ===\n",
    "with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "    texte = f.read()\n",
    "\n",
    "# === Param√®tres de traitement ===\n",
    "chunk_size = 200_000\n",
    "entites = []\n",
    "\n",
    "# === Si le fichier existe d√©j√†, on ne l‚Äô√©crase pas ===\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Texte;Type\\n\")\n",
    "\n",
    "# === Boucle par blocs ===\n",
    "for i in range(0, len(texte), chunk_size):\n",
    "    bloc = texte[i:i+chunk_size]\n",
    "    doc = nlp(bloc)\n",
    "    bloc_ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Sauvegarde imm√©diate du bloc dans le CSV\n",
    "    df_bloc = pd.DataFrame(bloc_ents, columns=[\"Texte\", \"Type\"])\n",
    "    df_bloc.to_csv(output_file, mode='a', index=False, sep=';', header=False)\n",
    "\n",
    "    print(f\"üíæ Bloc {i//chunk_size + 1} trait√© ({len(bloc):,} caract√®res, {len(bloc_ents)} entit√©s)\")\n",
    "    entites.extend(bloc_ents)\n",
    "\n",
    "print(f\"\\n‚úÖ Analyse termin√©e : {len(entites):,} entit√©s reconnues.\")\n",
    "print(f\"üíæ R√©sultat enregistr√© dans : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compter, trier et imprimer les entit√©s du corpus de 1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('doc' in locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m orgs = defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Parcourir toutes les entit√©s d√©tect√©es\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m.ents:\n\u001b[32m      8\u001b[39m     label = ent.label_\n\u001b[32m      9\u001b[39m     text = ent.text.strip()\n",
      "\u001b[31mNameError\u001b[39m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "# Cr√©er un dictionnaire pour chaque type d'entit√©\n",
    "people = defaultdict(int)\n",
    "places = defaultdict(int)\n",
    "orgs = defaultdict(int)\n",
    "\n",
    "# Parcourir toutes les entit√©s d√©tect√©es\n",
    "for ent in doc.ents:\n",
    "    label = ent.label_\n",
    "    text = ent.text.strip()\n",
    "    if len(text) <= 2:  # filtrer pour s√©lectionner les mots de minimum 3 caract√®res\n",
    "        continue\n",
    "\n",
    "    if label == \"PER\":\n",
    "        people[text] += 1\n",
    "    elif label == \"LOC\":\n",
    "        places[text] += 1\n",
    "    elif label == \"ORG\":\n",
    "        orgs[text] += 1\n",
    "\n",
    "# Trier les r√©sultats pour chaque type\n",
    "top_people = Counter(people).most_common(15)\n",
    "top_places = Counter(places).most_common(15)\n",
    "top_orgs = Counter(orgs).most_common(15)\n",
    "\n",
    "# Cr√©er un tableau r√©capitulatif (DataFrame)\n",
    "df_summary = pd.DataFrame({\n",
    "    \"Personnes\": [p[0] for p in top_people],\n",
    "    \"Fr√©quence (PER)\": [p[1] for p in top_people],\n",
    "    \"Lieux\": [p[0] for p in top_places],\n",
    "    \"Fr√©quence (LOC)\": [p[1] for p in top_places],\n",
    "    \"Organisations\": [p[0] for p in top_orgs],\n",
    "    \"Fr√©quence (ORG)\": [p[1] for p in top_orgs],\n",
    "})\n",
    "\n",
    "# Afficher le tableau\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th√©r√®se apparait 24 fois dans le corpus\n",
      "Guilbert apparait 22 fois dans le corpus\n",
      "M. Wilson apparait 20 fois dans le corpus\n",
      "Rossel apparait 20 fois dans le corpus\n",
      "Gioconda apparait 18 fois dans le corpus\n",
      "Moli√®re apparait 17 fois dans le corpus\n",
      "Enguerrand apparait 17 fois dans le corpus\n",
      "Wilson apparait 16 fois dans le corpus\n",
      "Daubrac apparait 15 fois dans le corpus\n",
      "Parentis apparait 13 fois dans le corpus\n",
      "Savinien apparait 13 fois dans le corpus\n",
      "Octave apparait 12 fois dans le corpus\n",
      "Agence Rossel apparait 12 fois dans le corpus\n",
      "Monsieur apparait 12 fois dans le corpus\n",
      "Edwige apparait 12 fois dans le corpus\n",
      "duc d‚ÄôAumale apparait 11 fois dans le corpus\n",
      "jo no apparait 9 fois dans le corpus\n",
      "Jules Ferry apparait 9 fois dans le corpus\n",
      "Naissances apparait 9 fois dans le corpus\n",
      "Reine apparait 9 fois dans le corpus\n",
      "Monseigneur apparait 9 fois dans le corpus\n",
      "Graux apparait 8 fois dans le corpus\n",
      "Finet apparait 8 fois dans le corpus\n",
      "Ali-Baba apparait 8 fois dans le corpus\n",
      "Henri apparait 8 fois dans le corpus\n",
      "Aubertin apparait 7 fois dans le corpus\n",
      "P Soc apparait 7 fois dans le corpus\n",
      "Pollet apparait 7 fois dans le corpus\n",
      "M. Van Praet apparait 7 fois dans le corpus\n",
      "prince Baudouin apparait 6 fois dans le corpus\n",
      "Guillaume apparait 6 fois dans le corpus\n",
      "Lundi apparait 6 fois dans le corpus\n",
      "M. Vigneau apparait 6 fois dans le corpus\n",
      "Messieurs apparait 6 fois dans le corpus\n",
      "Marguerite apparait 6 fois dans le corpus\n",
      "Simon apparait 6 fois dans le corpus\n",
      "Jo no apparait 5 fois dans le corpus\n",
      "Abon apparait 5 fois dans le corpus\n",
      "Cirque Royal apparait 5 fois dans le corpus\n",
      "Baudet apparait 5 fois dans le corpus\n",
      "Propri√©taires apparait 5 fois dans le corpus\n",
      "Grippe-Soleil apparait 5 fois dans le corpus\n",
      "Ambassadeurs apparait 5 fois dans le corpus\n",
      "A Houill apparait 5 fois dans le corpus\n",
      "Ponchielli apparait 5 fois dans le corpus\n",
      "Lucie apparait 5 fois dans le corpus\n",
      "Daubiehon apparait 5 fois dans le corpus\n",
      "madame apparait 5 fois dans le corpus\n",
      "Kilraine apparait 4 fois dans le corpus\n",
      "Mme X. apparait 4 fois dans le corpus\n"
     ]
    }
   ],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice: essayez de lister les lieux (LOC) et les organisations (ORG) les plus mentionn√©es dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
