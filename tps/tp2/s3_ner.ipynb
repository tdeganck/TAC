{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance d'entit√©s nomm√©es avec SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La documentation est accessible ici: https://spacy.io/api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 14:39:06.307063: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-04 14:39:07.936275: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "DEPRECATION: https://github.com/explosion/spacy-models/releases/download/-fr_core_news_md/-fr_core_news_md.tar.gz#egg===fr_core_news_md contains an egg fragment with a non-PEP 508 name. pip 25.3 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/13157\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '==fr_core_news_md': Expected package name at the start of dependency specifier\n",
      "    ==fr_core_news_md\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "!python -m spacy download fr_core_news_md\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconnaissance des entit√©s dans le corpus de 1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Bloc 1 trait√© (200,000 caract√®res, 3899 entit√©s)\n",
      "üíæ Bloc 2 trait√© (200,000 caract√®res, 2806 entit√©s)\n",
      "üíæ Bloc 3 trait√© (200,000 caract√®res, 3654 entit√©s)\n",
      "üíæ Bloc 4 trait√© (200,000 caract√®res, 4579 entit√©s)\n",
      "üíæ Bloc 5 trait√© (200,000 caract√®res, 5243 entit√©s)\n",
      "üíæ Bloc 6 trait√© (200,000 caract√®res, 5879 entit√©s)\n",
      "üíæ Bloc 7 trait√© (200,000 caract√®res, 4056 entit√©s)\n",
      "üíæ Bloc 8 trait√© (200,000 caract√®res, 4952 entit√©s)\n",
      "üíæ Bloc 9 trait√© (200,000 caract√®res, 5248 entit√©s)\n",
      "üíæ Bloc 10 trait√© (200,000 caract√®res, 5564 entit√©s)\n",
      "üíæ Bloc 11 trait√© (200,000 caract√®res, 3547 entit√©s)\n",
      "üíæ Bloc 12 trait√© (200,000 caract√®res, 3691 entit√©s)\n",
      "üíæ Bloc 13 trait√© (200,000 caract√®res, 4940 entit√©s)\n",
      "üíæ Bloc 14 trait√© (200,000 caract√®res, 4666 entit√©s)\n",
      "üíæ Bloc 15 trait√© (109,828 caract√®res, 1876 entit√©s)\n",
      "\n",
      "‚úÖ Analyse termin√©e : 64,600 entit√©s reconnues.\n",
      "üíæ R√©sultat enregistr√© dans : C:\\Users\\tommy\\TAC2\\TAC\\data\\txt\\entites_1965.csv\n"
     ]
    }
   ],
   "source": [
    "annee_choisie = \"1965\"\n",
    "folder_path = r\"C:\\Users\\tommy\\TAC2\\TAC\\data\\txt\"\n",
    "corpus_file = os.path.join(folder_path, f\"corpus_{annee_choisie}.txt\")\n",
    "output_file = os.path.join(folder_path, f\"entites_{annee_choisie}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Charger le texte complet de 1965 --> version trop longue !!! et sans sauvegarde !!!\n",
    "# corpus_file = os.path.join(folder_path, f\"corpus_{annee_choisie}.txt\")\n",
    "# with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "  #  texte = f.read()\n",
    "\n",
    "# Analyse linguistique compl√®te du corpus\n",
    "# nlp.max_length = 3_000_000  # ‚Üê augmente la limite √† 3 millions de caract√®res\n",
    "# doc = nlp(texte)\n",
    "\n",
    "# D√©couper le texte en phrases\n",
    "# sentences = list(doc.sents)\n",
    "\n",
    "# Parcourir chaque phrase et extraire les entit√©s\n",
    "# for sent in sentences:\n",
    "    # entities = [f\"{ent.text} ({ent.label_})\" for ent in sent.ents]\n",
    "    # if entities:\n",
    "        # print(f\"‚Üí '{sent.text.strip()}' contient les entit√©s : {', '.join(entities)}\")\n",
    "\n",
    "# refaire en d√©coupant en blocs pour gagner du temps\n",
    "# === Param√®tres ===\n",
    "# === Charger le mod√®le spaCy (NER uniquement) ===\n",
    "nlp = spacy.load(\"fr_core_news_md\", disable=[\"tagger\", \"parser\", \"lemmatizer\", \"attribute_ruler\"])\n",
    "nlp.max_length = 3_000_000\n",
    "\n",
    "# === Lire le texte complet ===\n",
    "with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "    texte = f.read()\n",
    "\n",
    "# === Param√®tres de traitement ===\n",
    "chunk_size = 200_000\n",
    "entites = []\n",
    "\n",
    "# === Si le fichier existe d√©j√†, on ne l‚Äô√©crase pas ===\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Texte;Type\\n\")\n",
    "\n",
    "# === Boucle par blocs ===\n",
    "for i in range(0, len(texte), chunk_size):\n",
    "    bloc = texte[i:i+chunk_size]\n",
    "    doc = nlp(bloc)\n",
    "    bloc_ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Sauvegarde imm√©diate du bloc dans le CSV\n",
    "    df_bloc = pd.DataFrame(bloc_ents, columns=[\"Texte\", \"Type\"])\n",
    "    df_bloc.to_csv(output_file, mode='a', index=False, sep=';', header=False)\n",
    "\n",
    "    print(f\"üíæ Bloc {i//chunk_size + 1} trait√© ({len(bloc):,} caract√®res, {len(bloc_ents)} entit√©s)\")\n",
    "    entites.extend(bloc_ents)\n",
    "\n",
    "print(f\"\\n‚úÖ Analyse termin√©e : {len(entites):,} entit√©s reconnues.\")\n",
    "print(f\"üíæ R√©sultat enregistr√© dans : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compter, trier et imprimer les entit√©s du corpus de 1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('doc' in locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Personnes  Fr√©quence (PER)       Lieux  Fr√©quence (LOC)  Organisations  \\\n",
      "0    Rossel              307   Bruxelles              551  Agence Rossel   \n",
      "1       Ecr              126        Brux              302         Mutuel   \n",
      "2  Monsieur              105    Belgique              135            bur   \n",
      "3      SENT               84      T√©l√©ph              101            dem   \n",
      "4     - T√©l               77       Paris               90            maz   \n",
      "5      trav               44       Li√®ge               87            chf   \n",
      "6     Bonne               43  Anderlecht               87           Fiat   \n",
      "7       T√©l               43        Etat               80       Standard   \n",
      "8  MONSIEUR               40      Anvers               79      T√©l√©phone   \n",
      "9   Moli√®re               38     Ixelles               67        Conseil   \n",
      "\n",
      "   Fr√©quence (ORG)  \n",
      "0              118  \n",
      "1               61  \n",
      "2               40  \n",
      "3               38  \n",
      "4               34  \n",
      "5               29  \n",
      "6               26  \n",
      "7               25  \n",
      "8               23  \n",
      "9               22  \n"
     ]
    }
   ],
   "source": [
    "# === Charger le CSV des entit√©s ===\n",
    "annee_choisie = \"1965\"\n",
    "folder_path = r\"C:\\Users\\tommy\\TAC2\\TAC\\data\\txt\"\n",
    "entites_file = os.path.join(folder_path, f\"entites_{annee_choisie}.csv\")\n",
    "\n",
    "df = pd.read_csv(entites_file, sep=';')\n",
    "\n",
    "# === Cr√©er des dictionnaires pour chaque type d'entit√© ===\n",
    "people = defaultdict(int)\n",
    "places = defaultdict(int)\n",
    "orgs   = defaultdict(int)\n",
    "\n",
    "# === Parcourir toutes les entit√©s du CSV ===\n",
    "for _, row in df.iterrows():\n",
    "    label = row[\"Type\"]\n",
    "    text = str(row[\"Texte\"]).strip()\n",
    "\n",
    "    if len(text) <= 2:  # filtrer les entit√©s trop courtes\n",
    "        continue\n",
    "\n",
    "    if label == \"PER\":\n",
    "        people[text] += 1\n",
    "    elif label == \"LOC\":\n",
    "        places[text] += 1\n",
    "    elif label == \"ORG\":\n",
    "        orgs[text] += 1\n",
    "\n",
    "# === Trier les r√©sultats (10 premiers de chaque type) ===\n",
    "top_people = Counter(people).most_common(10)\n",
    "top_places = Counter(places).most_common(10)\n",
    "top_orgs   = Counter(orgs).most_common(10)\n",
    "\n",
    "# === Cr√©er un tableau r√©capitulatif ===\n",
    "df_summary = pd.DataFrame({\n",
    "    \"Personnes\": [p[0] for p in top_people],\n",
    "    \"Fr√©quence (PER)\": [p[1] for p in top_people],\n",
    "    \"Lieux\": [p[0] for p in top_places],\n",
    "    \"Fr√©quence (LOC)\": [p[1] for p in top_places],\n",
    "    \"Organisations\": [p[0] for p in top_orgs],\n",
    "    \"Fr√©quence (ORG)\": [p[1] for p in top_orgs],\n",
    "})\n",
    "\n",
    "# === Afficher le tableau ===\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer la liste des entit√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Type         Texte_net  Fr√©quence\n",
      "45     LOC            France        108\n",
      "133    LOC             Paris         91\n",
      "1535   LOC             Li√®ge         88\n",
      "4339   LOC        Anderlecht         87\n",
      "200    LOC              Etat         80\n",
      "...    ...               ...        ...\n",
      "64580  PER    Pierre Vasnier          1\n",
      "64584  PER         Ir F- K J          1\n",
      "64590  PER              Mc M          1\n",
      "64594  PER          Verlaine          1\n",
      "64599  PER  M√©dicis U - Le K          1\n",
      "\n",
      "[27490 rows x 3 columns]\n",
      "\n",
      "üíæ Tableau complet export√© : C:\\Users\\tommy\\TAC2\\TAC\\data\\txt\\entites_nettoyees_1965.csv\n"
     ]
    }
   ],
   "source": [
    "# === Charger le CSV des entit√©s ===\n",
    "df = pd.read_csv(entites_file, sep=';')\n",
    "\n",
    "# === Liste de stopwords simples ===\n",
    "sw = {\n",
    "    \"le\",\"la\",\"les\",\"de\",\"du\",\"des\",\"d\",\"l\",\"au\",\"aux\",\"et\",\"ou\",\"√†\",\"en\",\"sur\",\"sous\",\n",
    "    \"un\",\"une\",\"pour\",\"par\",\"dans\",\"avec\",\"chez\",\"sans\",\"entre\",\"contre\",\"vers\",\n",
    "    \"ce\",\"cet\",\"cette\",\"ces\",\"se\",\"sa\",\"son\",\"leurs\",\"leur\",\"plus\",\"moins\",\"fait\", \"bruxelles\", \"belgique\", \"rue\", \"plus\", \"t√©l\", \"rossel\", \"ans\", \"deux\", \"tout\", \"cette\", \"van\", \"dem\", \"prix\", \"apr√®s\", \"bien\", \"sans\", \"tr√®s\", \"brux\", \"comme\", \"faire\", \n",
    "    \"faire\",\"√™tre\", \"sous\", \"heures\", \"grand\", \"√©crire\", \"soir\", \"tous\", \"fait\", \"part\", \"ecrire\", \"place\", \"demande\", \"maison\", \"jours\", \"dont\", \"app\", \"bon\", \"temps\", \n",
    "    \"avenue\", \"entre\", \"service\", \"encore\", \"gar\", \"aussi\", \"leurs\", \"non\", \"contre\", \"premi√®re\", \"avant\", \"bonne\", \"peut\", \"mois\", \"lieu\", \"peu\", \"autre\", \"ecr\", \"jeune\", \n",
    "    \"jour\", \"jour\", \"samedi\", \"lundi\", \"mardi\", \"mercredi\", \"jeudi\", \"vendredi\", \"samedi\", \"dimanche\", \"janvier\", \"f√©vrier\", \"mars\", \"avril\", \"mai\", \"ao√ªt\", \"septembre\", \"octobre\",\n",
    "    \"novembre\", \"d√©cembre\",  \"autres\", \"t√©l√©ph\", \"monsieur\", \"pr√©s\", \"grande\", \"moins\", \"pays\", \"midi\", \"madame\", \"dimanche\", \"cours\", \"toutes\", \"semaine\", \"ainsi\", \n",
    "    \"toute\", \"premier\", \"dit\", \"francs\", \"quelques\", \"quelque\", \"fois\", \"importante\", \"cuis\", \"etc\", \"vente\", \"terr\", \"jeudi\", \"conf\", \"avoir\", \"jeunes\", \"depuis\", \"chauss√©e\", \n",
    "     \"vers\", \"ann√©e\", \"juin\", \"juillet\", \"mai\", \"d√©j√†\", \"chez\", \"d√®s\", \"cet\", \"mercredi\", \"jusqu\", \"cherche\", \"pr√®s\", \"mod\", \"louer\", \"partie\", \"celui\", \"belle\", \"fin\", \n",
    "     \"vendre\", \"engage\", \"bel\", \"alors\", \"toujours\", \"petit\", \"suite\", \"partir\", \"ceux\", \"dire\", \"trav\", \"faut\", \"car\", \"rez\", \"devant\", \"jard\", \"celle\", \"doit\", \"frs\", \n",
    "    \"rien\", \"dernier\", \"num√©ro\", \"tel\", \"beau\", \"chaque\", \"elles\", \"je\", \"tu\", \"vous\", \"il\", \"nous\", \"ils\", \"elles\", \"t√©l√©phone\", \"petits\", \"points\", \"cela\", \"nouvelle\", \"donc\", \n",
    "     \"aff\", \"voir\", \"plusieurs\", \"trop\", \"beaux\", \"quand\", \"assez\", \"demi\", \"haut\", \"gros\", \"ann√©es\", \"heure\", \"bur\", \"vend\", \"cependant\", \"six\", \"puis\", \"seul\", \"cas\", \"parmi\", \n",
    "     \"h√¥tel\", \"recherche\", \"appart\", \"beaucoup\", \"petite\", \"pers\", \"bat\", \"prendre\", \"grd\", \"deuxi√®me\", \"troisi√®me\", \"quatre\", \"cinq\", \"sept\", \"huit\", \"neuf\", \"dix\", \"sent\", \"√©galement\", \n",
    "    \"nouveau\", \"bas\", \"pendant\", \"ici\", \"l√†\", \"txt\", \"tant\",  \"courant\", \"surtout\", \"rem\", \"fa√ß\", \"bonnes\", \"minutes\", \"jamais\", \"enfin\", \"bons\", \"certains\", \"mieux\", \"quart\", \n",
    "    \"seulement\", \"voit\", \"maz\", \"mat\", \"d√©s\", \"sem\", \"poss\", \"mise\", \"notamment\",\"villa\", \"services\", \"bureau\", \"bureaux\", \"jardin\", \"chambres\",\"agence\", \"vue\", \"trois\", \"garage\", \"march√©\", \n",
    "    \"fit\", \"adresser\", \"serie\", \"adr\", \"imm\", \"com\", \"peuvent\", \"agit\", \"selon\", \"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"√™tre\", \"deux\", \"comme\", \"dont\", \"tout\", \"ils\", \"bien\", \"sans\", \"peut\", \n",
    "    \"tous\", \"apr√®s\", \"ainsi\", \"donc\", \"cet\", \"sous\",\"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\", \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \n",
    "    \"depuis\", \"autres\",\"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"t√©l\", \"Soir\", \"ecr\", \"rue\", \"bon\", \"Bruxelles\",\"bruxelles\", \"prix\", \"Brux\", \"ans\", \"maison\", \"√©crire\", \"pr√®s\", \n",
    "    \"peu\", \"d√©s\", \"ecrire\", \"Brux\", \"brux\", \"part\", \"grand\", \"vendre\", \"tr√®s\", \"vend\", \"pr√©s\", \"mod\", \"etc\", \"avant\", \"pet\", \"cherche\", \"vente\"\n",
    "}\n",
    "\n",
    "def norm_unicode(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "def clean_entity(text: str, label: str) -> str | None:\n",
    "    \"\"\"Nettoie une entit√© extraite par spaCy (supprime stopwords, ponctuation, etc.)\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    t = norm_unicode(text).strip()\n",
    "    # Retire ponctuation en bord\n",
    "    t = re.sub(r\"^[^\\w]+|[^\\w]+$\", \"\", t)\n",
    "    if not t:\n",
    "        return None\n",
    "    toks = re.split(r\"\\s+\", t)\n",
    "    # Si tous les tokens sont des stopwords, on ignore\n",
    "    if all(tok.lower() in sw for tok in toks):\n",
    "        return None\n",
    "    # Retire les stopwords au d√©but et √† la fin\n",
    "    while toks and toks[0].lower() in sw:\n",
    "        toks.pop(0)\n",
    "    while toks and toks[-1].lower() in sw:\n",
    "        toks.pop()\n",
    "    if not toks:\n",
    "        return None\n",
    "    t = \" \".join(toks)\n",
    "    if label == \"PER\":\n",
    "        t = t.title()\n",
    "    else:\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    if len(t) <= 2:\n",
    "        return None\n",
    "    return t\n",
    "\n",
    "# === Appliquer le nettoyage aux entit√©s existantes ===\n",
    "df[\"Texte_net\"] = [clean_entity(txt, lbl) for txt, lbl in zip(df[\"Texte\"], df[\"Type\"])]\n",
    "\n",
    "# === Garder uniquement les entit√©s valides ===\n",
    "df_clean = df[df[\"Texte_net\"].notna() & df[\"Type\"].isin([\"PER\", \"LOC\", \"ORG\"])].copy()\n",
    "\n",
    "# === Calculer la fr√©quence de chaque entit√© ===\n",
    "df_clean[\"Fr√©quence\"] = df_clean.groupby([\"Type\", \"Texte_net\"])[\"Texte_net\"].transform(\"count\")\n",
    "df_clean = df_clean.drop_duplicates(subset=[\"Type\", \"Texte_net\"])\n",
    "df_clean = df_clean.sort_values([\"Type\", \"Fr√©quence\"], ascending=[True, False])\n",
    "\n",
    "# === Afficher le tableau complet ===\n",
    "print(df_clean[[\"Type\", \"Texte_net\", \"Fr√©quence\"]])\n",
    "\n",
    "# === (Optionnel) Sauvegarder le tableau complet ===\n",
    "output_file = os.path.join(folder_path, f\"entites_nettoyees_{annee_choisie}.csv\")\n",
    "df_clean.to_csv(output_file, index=False, sep=\";\")\n",
    "print(f\"\\nüíæ Tableau complet export√© : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compter, trier et afficher les entit√©s nettoy√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Personnes  Fr√©quence (PER)       Lieux  Fr√©quence (LOC) Organisations  \\\n",
      "0   Moli√®re                1      France                1        Mutuel   \n",
      "1       Roi                1       Paris                1           chf   \n",
      "2   Jacques                1       Li√®ge                1          Fiat   \n",
      "3       Enf                1  Anderlecht                1      Standard   \n",
      "4      Curr                1        Etat                1       Conseil   \n",
      "5     S.D.B                1      Anvers                1    ANDERLECHT   \n",
      "6    Louise                1        Gand                1     Parlement   \n",
      "7    Mozart                1     Ixelles                1      Couronne   \n",
      "8    Tt Cft                1       Uccle                1           s.b   \n",
      "9       Emp                1       Namur                1          FIAT   \n",
      "\n",
      "   Fr√©quence (ORG)  \n",
      "0                1  \n",
      "1                1  \n",
      "2                1  \n",
      "3                1  \n",
      "4                1  \n",
      "5                1  \n",
      "6                1  \n",
      "7                1  \n",
      "8                1  \n",
      "9                1  \n"
     ]
    }
   ],
   "source": [
    "clean_file = os.path.join(folder_path, f\"entites_nettoyees_{annee_choisie}.csv\")\n",
    "\n",
    "# === Charger le CSV nettoy√© ===\n",
    "df = pd.read_csv(clean_file, sep=';')\n",
    "\n",
    "# Choisir la bonne colonne de texte (nettoy√©e si pr√©sente)\n",
    "col_txt = \"Texte_net\" if \"Texte_net\" in df.columns else \"Texte\"\n",
    "\n",
    "# === Compter par type ===\n",
    "people = Counter(df.loc[df[\"Type\"]==\"PER\", col_txt])\n",
    "places = Counter(df.loc[df[\"Type\"]==\"LOC\", col_txt])\n",
    "orgs   = Counter(df.loc[df[\"Type\"]==\"ORG\", col_txt])\n",
    "\n",
    "# === Top N (ajuste comme tu veux) ===\n",
    "topN = 10\n",
    "top_people = people.most_common(topN)\n",
    "top_places = places.most_common(topN)\n",
    "top_orgs   = orgs.most_common(topN)\n",
    "\n",
    "# === Tableau r√©capitulatif ===\n",
    "df_summary = pd.DataFrame({\n",
    "    \"Personnes\": [p for p,_ in top_people],\n",
    "    \"Fr√©quence (PER)\": [n for _,n in top_people],\n",
    "    \"Lieux\": [p for p,_ in top_places],\n",
    "    \"Fr√©quence (LOC)\": [n for _,n in top_places],\n",
    "    \"Organisations\": [p for p,_ in top_orgs],\n",
    "    \"Fr√©quence (ORG)\": [n for _,n in top_orgs],\n",
    "})\n",
    "\n",
    "print(df_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
