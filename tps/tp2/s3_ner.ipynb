{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance d'entit√©s nomm√©es avec SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La documentation est accessible ici: https://spacy.io/api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 14:39:06.307063: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-04 14:39:07.936275: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "DEPRECATION: https://github.com/explosion/spacy-models/releases/download/-fr_core_news_md/-fr_core_news_md.tar.gz#egg===fr_core_news_md contains an egg fragment with a non-PEP 508 name. pip 25.3 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/13157\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '==fr_core_news_md': Expected package name at the start of dependency specifier\n",
      "    ==fr_core_news_md\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "!python -m spacy download fr_core_news_md\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconnaissance des entit√©s dans le corpus de 1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Bloc 1 trait√© (200,000 caract√®res, 3899 entit√©s)\n",
      "üíæ Bloc 2 trait√© (200,000 caract√®res, 2806 entit√©s)\n",
      "üíæ Bloc 3 trait√© (200,000 caract√®res, 3654 entit√©s)\n",
      "üíæ Bloc 4 trait√© (200,000 caract√®res, 4579 entit√©s)\n",
      "üíæ Bloc 5 trait√© (200,000 caract√®res, 5243 entit√©s)\n",
      "üíæ Bloc 6 trait√© (200,000 caract√®res, 5879 entit√©s)\n",
      "üíæ Bloc 7 trait√© (200,000 caract√®res, 4056 entit√©s)\n",
      "üíæ Bloc 8 trait√© (200,000 caract√®res, 4952 entit√©s)\n",
      "üíæ Bloc 9 trait√© (200,000 caract√®res, 5248 entit√©s)\n",
      "üíæ Bloc 10 trait√© (200,000 caract√®res, 5564 entit√©s)\n",
      "üíæ Bloc 11 trait√© (200,000 caract√®res, 3547 entit√©s)\n",
      "üíæ Bloc 12 trait√© (200,000 caract√®res, 3691 entit√©s)\n",
      "üíæ Bloc 13 trait√© (200,000 caract√®res, 4940 entit√©s)\n",
      "üíæ Bloc 14 trait√© (200,000 caract√®res, 4666 entit√©s)\n",
      "üíæ Bloc 15 trait√© (109,828 caract√®res, 1876 entit√©s)\n",
      "\n",
      "‚úÖ Analyse termin√©e : 64,600 entit√©s reconnues.\n",
      "üíæ R√©sultat enregistr√© dans : C:\\Users\\tommy\\TAC2\\TAC\\data\\txt\\entites_1965.csv\n"
     ]
    }
   ],
   "source": [
    "annee_choisie = \"1965\"\n",
    "folder_path = r\"C:\\Users\\tommy\\TAC2\\TAC\\data\\txt\"\n",
    "corpus_file = os.path.join(folder_path, f\"corpus_{annee_choisie}.txt\")\n",
    "output_file = os.path.join(folder_path, f\"entites_{annee_choisie}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Charger le texte complet de 1965 --> version trop longue !!! et sans sauvegarde !!!\n",
    "# corpus_file = os.path.join(folder_path, f\"corpus_{annee_choisie}.txt\")\n",
    "# with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "  #  texte = f.read()\n",
    "\n",
    "# Analyse linguistique compl√®te du corpus\n",
    "# nlp.max_length = 3_000_000  # ‚Üê augmente la limite √† 3 millions de caract√®res\n",
    "# doc = nlp(texte)\n",
    "\n",
    "# D√©couper le texte en phrases\n",
    "# sentences = list(doc.sents)\n",
    "\n",
    "# Parcourir chaque phrase et extraire les entit√©s\n",
    "# for sent in sentences:\n",
    "    # entities = [f\"{ent.text} ({ent.label_})\" for ent in sent.ents]\n",
    "    # if entities:\n",
    "        # print(f\"‚Üí '{sent.text.strip()}' contient les entit√©s : {', '.join(entities)}\")\n",
    "\n",
    "# refaire en d√©coupant en blocs pour gagner du temps\n",
    "# === Param√®tres ===\n",
    "# === Charger le mod√®le spaCy (NER uniquement) ===\n",
    "nlp = spacy.load(\"fr_core_news_md\", disable=[\"tagger\", \"parser\", \"lemmatizer\", \"attribute_ruler\"])\n",
    "nlp.max_length = 3_000_000\n",
    "\n",
    "# === Lire le texte complet ===\n",
    "with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "    texte = f.read()\n",
    "\n",
    "# === Param√®tres de traitement ===\n",
    "chunk_size = 200_000\n",
    "entites = []\n",
    "\n",
    "# === Si le fichier existe d√©j√†, on ne l‚Äô√©crase pas ===\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Texte;Type\\n\")\n",
    "\n",
    "# === Boucle par blocs ===\n",
    "for i in range(0, len(texte), chunk_size):\n",
    "    bloc = texte[i:i+chunk_size]\n",
    "    doc = nlp(bloc)\n",
    "    bloc_ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Sauvegarde imm√©diate du bloc dans le CSV\n",
    "    df_bloc = pd.DataFrame(bloc_ents, columns=[\"Texte\", \"Type\"])\n",
    "    df_bloc.to_csv(output_file, mode='a', index=False, sep=';', header=False)\n",
    "\n",
    "    print(f\"üíæ Bloc {i//chunk_size + 1} trait√© ({len(bloc):,} caract√®res, {len(bloc_ents)} entit√©s)\")\n",
    "    entites.extend(bloc_ents)\n",
    "\n",
    "print(f\"\\n‚úÖ Analyse termin√©e : {len(entites):,} entit√©s reconnues.\")\n",
    "print(f\"üíæ R√©sultat enregistr√© dans : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compter, trier et imprimer les entit√©s du corpus de 1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('doc' in locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Personnes  Fr√©quence (PER)       Lieux  Fr√©quence (LOC)  Organisations  \\\n",
      "0      Rossel              307   Bruxelles              551  Agence Rossel   \n",
      "1         Ecr              126        Brux              302         Mutuel   \n",
      "2    Monsieur              105    Belgique              135            bur   \n",
      "3        SENT               84      T√©l√©ph              101            dem   \n",
      "4       - T√©l               77       Paris               90            maz   \n",
      "5        trav               44       Li√®ge               87            chf   \n",
      "6       Bonne               43  Anderlecht               87           Fiat   \n",
      "7         T√©l               43        Etat               80       Standard   \n",
      "8    MONSIEUR               40      Anvers               79      T√©l√©phone   \n",
      "9     Moli√®re               38     Ixelles               67        Conseil   \n",
      "10        Sem               31        Gand               66            adr   \n",
      "11        Roi               31       Uccle               60       Couronne   \n",
      "12    Jacques               31      France               57           FIAT   \n",
      "13     Ecrire               30   Allemagne               53     ANDERLECHT   \n",
      "14  Ecrire Ag               27     Londres               52      Parlement   \n",
      "\n",
      "    Fr√©quence (ORG)  \n",
      "0               118  \n",
      "1                61  \n",
      "2                40  \n",
      "3                38  \n",
      "4                34  \n",
      "5                29  \n",
      "6                26  \n",
      "7                25  \n",
      "8                23  \n",
      "9                22  \n",
      "10               21  \n",
      "11               19  \n",
      "12               19  \n",
      "13               18  \n",
      "14               18  \n"
     ]
    }
   ],
   "source": [
    "# === Charger le CSV des entit√©s ===\n",
    "annee_choisie = \"1965\"\n",
    "folder_path = r\"C:\\Users\\tommy\\TAC2\\TAC\\data\\txt\"\n",
    "entites_file = os.path.join(folder_path, f\"entites_{annee_choisie}.csv\")\n",
    "\n",
    "df = pd.read_csv(entites_file, sep=';')\n",
    "\n",
    "# === Cr√©er des dictionnaires pour chaque type d'entit√© ===\n",
    "people = defaultdict(int)\n",
    "places = defaultdict(int)\n",
    "orgs   = defaultdict(int)\n",
    "\n",
    "# === Parcourir toutes les entit√©s du CSV ===\n",
    "for _, row in df.iterrows():\n",
    "    label = row[\"Type\"]\n",
    "    text = str(row[\"Texte\"]).strip()\n",
    "\n",
    "    if len(text) <= 2:  # filtrer les entit√©s trop courtes\n",
    "        continue\n",
    "\n",
    "    if label == \"PER\":\n",
    "        people[text] += 1\n",
    "    elif label == \"LOC\":\n",
    "        places[text] += 1\n",
    "    elif label == \"ORG\":\n",
    "        orgs[text] += 1\n",
    "\n",
    "# === Trier les r√©sultats (15 premiers de chaque type) ===\n",
    "top_people = Counter(people).most_common(15)\n",
    "top_places = Counter(places).most_common(15)\n",
    "top_orgs   = Counter(orgs).most_common(15)\n",
    "\n",
    "# === Cr√©er un tableau r√©capitulatif ===\n",
    "df_summary = pd.DataFrame({\n",
    "    \"Personnes\": [p[0] for p in top_people],\n",
    "    \"Fr√©quence (PER)\": [p[1] for p in top_people],\n",
    "    \"Lieux\": [p[0] for p in top_places],\n",
    "    \"Fr√©quence (LOC)\": [p[1] for p in top_places],\n",
    "    \"Organisations\": [p[0] for p in top_orgs],\n",
    "    \"Fr√©quence (ORG)\": [p[1] for p in top_orgs],\n",
    "})\n",
    "\n",
    "# === Afficher le tableau ===\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
